{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 05 Lab - Data Preparation\n",
        "\n",
        "**Objective:** To learn and apply the most common data preparation techniques. Raw data is rarely ready for a machine learning model. This process, also called preprocessing, is one of the most critical steps in the entire ML workflow.\n",
        "\n",
        "**In this lab, you will write more of the code.** Read the explanations and then complete the tasks in the code cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Initial Look\n",
        "\n",
        "We will continue using the Titanic dataset because it has the exact problems we need to solve: missing values and non-numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Missing Values Before Cleaning ---\n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "\n",
        "# Let's look at the missing values\n",
        "print(\"--- Missing Values Before Cleaning ---\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Handling Missing Values (Imputation)\n",
        "\n",
        "**Concept:** Most machine learning models cannot handle missing values (`NaN`). We must deal with them. Dropping the rows is an option, but you lose data. A better way is **imputation**, which means filling in the missing values with a calculated guess.\n",
        "\n",
        "Common imputation strategies:\n",
        "*   **Mean:** Fill with the average value. Good for normally distributed data.\n",
        "*   **Median:** Fill with the middle value. Better for skewed data or data with outliers (like `Fare`).\n",
        "*   **Mode:** Fill with the most frequent value. Used for categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Impute the 'Age' Column\n",
        "\n",
        "The 'Age' column is missing many values. Since age can be skewed (e.g., by a few very old passengers), using the **median** is a robust choice.\n",
        "\n",
        "**Your Task:** Calculate the median of the 'Age' column and use the `.fillna()` method to replace the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in 'Age' after imputation:\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# --- ENTER YOUR CODE HERE ---\n",
        "\n",
        "# 1. Calculate the median of the 'Age' column\n",
        "median_age = df['Age'].median()\n",
        "\n",
        "# 2. Fill the missing values in 'Age' with the median\n",
        "df['Age'] = df['Age'].fillna(median_age)\n",
        "\n",
        "# 3. Verify that there are no more missing values in 'Age'\n",
        "\n",
        "print(\"Missing values in 'Age' after imputation:\")\n",
        "print(df['Age'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Encoding Categorical Features\n",
        "\n",
        "**Concept:** Machine learning models are mathematical, so they need numbers, not text. We need to convert categorical columns (like 'Sex' and 'Embarked') into a numerical format. The most common method is **One-Hot Encoding**.\n",
        "\n",
        "One-Hot Encoding takes a column with `N` categories and turns it into `N` new columns, each with a `1` or `0`. For example, the 'Sex' column (`male`, `female`) becomes two new columns: `Sex_male` and `Sex_female`.\n",
        "\n",
        "Pandas has a convenient function called `pd.get_dummies()` that does this for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: One-Hot Encode Categorical Columns\n",
        "\n",
        "**Your Task:** Use `pd.get_dummies()` to encode the 'Sex' and 'Embarked' columns. Make sure to drop the original columns after encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name   Age  SibSp  Parch  \\\n",
            "0                            Braund, Mr. Owen Harris  22.0      1      0   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n",
            "2                             Heikkinen, Miss. Laina  26.0      0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n",
            "4                           Allen, Mr. William Henry  35.0      0      0   \n",
            "\n",
            "             Ticket     Fare Cabin  Sex_male  Embarked_Q  Embarked_S  \n",
            "0         A/5 21171   7.2500   NaN      True       False        True  \n",
            "1          PC 17599  71.2833   C85     False       False       False  \n",
            "2  STON/O2. 3101282   7.9250   NaN     False       False        True  \n",
            "3            113803  53.1000  C123     False       False        True  \n",
            "4            373450   8.0500   NaN      True       False        True  \n"
          ]
        }
      ],
      "source": [
        "# --- ENTER YOUR CODE HERE ---\n",
        "\n",
        "# 1. Use get_dummies to create new columns for 'Sex' and 'Embarked'\n",
        "#    Set `drop_first=True` to avoid multicollinearity (a statistical issue), which drops one of the new columns (e.g., just having `Sex_male` is enough to know if someone is female).\n",
        "encoded_df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# 2. Display the first few rows of the new DataFrame to see the new columns\n",
        "print(encoded_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Feature Scaling\n",
        "\n",
        "**Concept:** Many models are sensitive to the scale of the features. For example, `Age` (from 0-80) and `Fare` (from 0-512) are on very different scales. This can cause the model to incorrectly believe that `Fare` is a more important feature simply because its values are larger.\n",
        "\n",
        "**Feature Scaling** solves this by putting all features on a similar scale. A common method is **Standardization** (`StandardScaler` in scikit-learn), which rescales the data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "**Important:** You only scale your numerical features, not your target variable or your newly encoded categorical columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Scale the 'Age' and 'Fare' Columns\n",
        "\n",
        "**Your Task:** Use `StandardScaler` from `sklearn.preprocessing` to scale the 'Age' and 'Fare' columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name       Age  SibSp  Parch  \\\n",
            "0                            Braund, Mr. Owen Harris -0.565736      1      0   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  0.663861      1      0   \n",
            "2                             Heikkinen, Miss. Laina -0.258337      0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  0.433312      1      0   \n",
            "4                           Allen, Mr. William Henry  0.433312      0      0   \n",
            "\n",
            "             Ticket      Fare Cabin  Sex_male  Embarked_Q  Embarked_S  \n",
            "0         A/5 21171 -0.502445   NaN      True       False        True  \n",
            "1          PC 17599  0.786845   C85     False       False       False  \n",
            "2  STON/O2. 3101282 -0.488854   NaN     False       False        True  \n",
            "3            113803  0.420730  C123     False       False        True  \n",
            "4            373450 -0.486337   NaN      True       False        True  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- ENTER YOUR CODE HERE ---\n",
        "\n",
        "# 1. Create an instance of the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Select the columns to scale\n",
        "columns_to_scale = ['Age', 'Fare']\n",
        "\n",
        "# 3. Fit the scaler to the data and transform it\n",
        "#    Note: We are using the `encoded_df` from the previous step if you created it.\n",
        "encoded_df[columns_to_scale] = scaler.fit_transform(encoded_df[columns_to_scale])\n",
        "\n",
        "# 4. Display the first few rows to see the scaled data\n",
        "print(encoded_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Knowledge Check\n",
        "\n",
        "**Instructions:** Answer the following questions in this markdown cell.\n",
        "\n",
        "1.  **Why is it often better to impute missing values with the median instead of the mean?**\n",
        "2.  **Explain in your own words what One-Hot Encoding does and why it is necessary.**\n",
        "3.  **Would you need to apply Feature Scaling to a Decision Tree model?** Why or why not? (Hint: Think about how a Decision Tree makes its splits).\n",
        "\n",
        "**[ENTER YOUR ANSWERS HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d107eea7",
      "metadata": {},
      "source": [
        "1. Meadian is better to use them the mean because mean is more impacted by outliiers and median is more realistic to a central value.\n",
        "2. One-Hot Encoding takes a value and puts it into a binary column. For example, Embark after encoding got split up into Embarked_Q and Embarked_S, both biniry columns. The first column (which would have eben Embarked_C) was dropped because we told it to drop the virst column. This make grouping and perdicting easier, as a binard value is easier to work with then having multiple choices.\n",
        "3. I don't beleive that it is necessary because decision tree splits based on threshholds, and does not need to be \"scaled\" down. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
